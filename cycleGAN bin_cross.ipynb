{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CycleGAN Training\n",
    "\n",
    "This notebook will train a cycleGAN to make simulated images look like experimental (microscopy) images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules\n",
    "First make sure all the modules are properly loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "from src.make_dataset import parse_and_save_dir, load_train_data\n",
    "from src.models import unet_generator, discriminator, generator_resnet\n",
    "import tensorflow as tf\n",
    "from src.plotting_tools import *\n",
    "import pickle\n",
    "import time\n",
    "from glob import glob\n",
    "from IPython.display import clear_output\n",
    "import scipy.fftpack as sfft\n",
    "from sys import argv\n",
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing parameters\n",
    "\n",
    "Next, we want to locate paths for the simulated images and the experimental images as well as name the cyclegan models:\n",
    "\n",
    "`identifier`: This will be the name of the model that will be useful for saving and loading checkpoints\n",
    "\n",
    "`parent_dir`: Location of the `cycle_gan` repository\n",
    "\n",
    "`exp_dir_raw`: The path to the folder containing experimental images\n",
    "\n",
    "`sim_dir_raw`: The path to the folder containing simulated images\n",
    "\n",
    "`gaussian`: Set this parameter to some number if you want to add gaussian noise with a given stddev to the simulation data\n",
    "\n",
    "`num_channels`: The number of channels of the experimental images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier = \"sample_data\"\n",
    "\n",
    "gaussian   = 0.1 \n",
    "\n",
    "parent_dir = \".../cyclegan/\"\n",
    "\n",
    "exp_dir_raw = parent_dir + \"data/exp/sample_data_exp/\"\n",
    "sim_dir_raw = parent_dir + \"data/sim/sample_data_synth/\"\n",
    "\n",
    "exp_dir = exp_dir_raw[:-1] + \"_256_slices/\"\n",
    "sim_dir = sim_dir_raw[:-1] + \"_gauss_{}_256_slices/\".format(gaussian)\n",
    "\n",
    "num_channels=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, set training hyperparameters:\n",
    "\n",
    "`fine_size`: the size of the image that will be fed into the model. \n",
    "\n",
    "`stride`: Each image in the folder will be cut into subimages that are `fine_size x fine_size` dimensional. This parameter is the stride length of the cuts made to the original image. \n",
    "\n",
    "`lr0`: Initial learning rate \n",
    "\n",
    "`ls0`: Initial learning rate for simulated generator and discriminator\n",
    "\n",
    "`le0`: Initial learning rate for experimential generator and discriminator\n",
    "\n",
    "`lam_cycle`: multiplier for the cycle loss\n",
    "\n",
    "`lam_ident`: multiplier for the identity loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_size, stride = 256, 64\n",
    "lr0 = 2e-5\n",
    "ls0=2e-6\n",
    "le0=2e-6\n",
    "lam_cycle = 10\n",
    "lam_ident = 0.5*lam_cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll the images into sub images to be fed into the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"raw directories:\")\n",
    "print(exp_dir_raw)\n",
    "print(sim_dir_raw)\n",
    "print(\"256 directories:\")\n",
    "print(exp_dir)\n",
    "print(sim_dir)\n",
    "\n",
    "#comment out last two lines when resuming from checkpoint\n",
    "print(\"parsing directories\")\n",
    "parse_and_save_dir(exp_dir_raw, exp_dir, fine_size, stride)\n",
    "parse_and_save_dir(sim_dir_raw, sim_dir, fine_size, stride, num_channels=num_channels, gaussian=gaussian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing models and losses\n",
    "\n",
    "Here we make the generators and discriminators as well as the loss functions. Feel free to change this if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Construct Generator and Discriminator\n",
    "# Here, we can choose between a unet generator or a resnet generator.\n",
    "\n",
    "generator_exp = unet_generator(num_channels, 1, \"instancenorm\")\n",
    "generator_sim = unet_generator(1, num_channels, \"instancenorm\")\n",
    "\n",
    "discriminator_sim = discriminator(\"instancenorm\", num_channels)\n",
    "discriminator_exp = discriminator(\"instancenorm\", 1)\n",
    "\n",
    "discriminator_sim_fft = discriminator(\"instancenorm\", num_channels)\n",
    "discriminator_exp_fft = discriminator(\"instancenorm\", 1)\n",
    "\n",
    "\n",
    "generator_exp_optimizer = tf.keras.optimizers.Adam(lr0)\n",
    "generator_sim_optimizer = tf.keras.optimizers.Adam(lr0)\n",
    "\n",
    "discriminator_sim_optimizer = tf.keras.optimizers.Adam(lr0)\n",
    "discriminator_exp_optimizer = tf.keras.optimizers.Adam(lr0)\n",
    "\n",
    "discriminator_sim_fft_optimizer = tf.keras.optimizers.Adam(lr0)\n",
    "discriminator_exp_fft_optimizer = tf.keras.optimizers.Adam(lr0)\n",
    "\n",
    "\n",
    "# ## Loss Functions and Accuracies\n",
    "\n",
    "loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "#loss_obj = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "def discriminator_loss(real, fake):\n",
    "    real_loss = loss_obj( tf.ones_like(real), real)\n",
    "    fake_loss = loss_obj(tf.zeros_like(fake), fake)\n",
    "    total_disc_loss = (real_loss*len(real) + fake_loss*len(fake))/(len(real) + len(fake))\n",
    "    return total_disc_loss\n",
    "\n",
    "def generator_loss(fake):\n",
    "    return loss_obj(tf.ones_like(fake), fake)\n",
    "\n",
    "def L1_loss(img_A, img_B):\n",
    "    return tf.reduce_mean(tf.abs(img_A - img_B))\n",
    "\n",
    "def L2_loss(img_A, img_B):\n",
    "    diff = tf.abs(img_A - img_B)\n",
    "    return tf.reduce_mean(diff*diff)\n",
    "\n",
    "\n",
    "# ## Checkpoints\n",
    "checkpoint_path = parent_dir + \"cycle_gan_results/checkpoints/checkpoint_{}\".format(identifier)\n",
    "log_data_fn = \"{}/data_{}.p\".format(checkpoint_path, identifier)\n",
    "\n",
    "ckpt = tf.train.Checkpoint(generator_exp=generator_exp,\n",
    "                           generator_sim=generator_sim,\n",
    "                           discriminator_sim=discriminator_sim,\n",
    "                           discriminator_exp=discriminator_exp,\n",
    "                           discriminator_sim_fft=discriminator_sim_fft,\n",
    "                           discriminator_exp_fft=discriminator_exp_fft,\n",
    "                           generator_exp_optimizer=generator_exp_optimizer,\n",
    "                           generator_sim_optimizer=generator_sim_optimizer,\n",
    "                           discriminator_sim_optimizer=discriminator_sim_optimizer,\n",
    "                           discriminator_exp_optimizer=discriminator_exp_optimizer,\n",
    "                           discriminator_sim_fft_optimizer=discriminator_sim_fft_optimizer,\n",
    "                           discriminator_exp_fft_optimizer=discriminator_exp_fft_optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path,max_to_keep=None)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    #cpath = ckpt_manager.checkpoints[-2]\n",
    "    cpath = ckpt_manager.latest_checkpoint\n",
    "    print(\"loading checkpoint \", cpath)\n",
    "    ckpt.restore(cpath)\n",
    "    print ('Latest checkpoint restored!')\n",
    "\n",
    "\n",
    "# ## Training Functions\n",
    "@tf.function\n",
    "def get_logf2(img_arr):\n",
    "    fft_list = tf.signal.fftshift(tf.signal.fft2d(tf.cast(tf.reshape(img_arr, [-1, fine_size, fine_size]), tf.complex64)))\n",
    "    re_list, im_list = tf.math.real(fft_list), tf.math.imag(fft_list)\n",
    "    f2_list = tf.math.multiply(re_list, re_list) + tf.math.multiply(im_list, im_list)\n",
    "    return tf.reshape(tf.math.log(tf.clip_by_value(f2_list, 1e-36, 1e36))/tf.math.log(10.), [-1, fine_size, fine_size, 1])\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_generators(real_sim, real_exp, train=True):\n",
    "    # persistent is set to True because the tape is used more than\n",
    "    # once to calculate the gradients.\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "    # Generator G translates X -> Y\n",
    "    # Generator F translates Y -> X.\n",
    "\n",
    "        #Generate fake and cycled images\n",
    "        fake_exp   = generator_exp(real_sim, training=train)\n",
    "        cycled_sim = generator_sim(fake_exp, training=train)\n",
    "\n",
    "        fake_sim   = generator_sim(real_exp, training=train)\n",
    "        cycled_exp = generator_exp(fake_sim, training=train)\n",
    "\n",
    "        same_sim   = generator_sim(real_sim, training=train)\n",
    "        same_exp   = generator_exp(real_exp, training=train)\n",
    "\n",
    "        #discriminate images\n",
    "        disc_fake_sim = discriminator_sim(fake_sim, training=False)\n",
    "        disc_fake_exp = discriminator_exp(fake_exp, training=False)\n",
    "\n",
    "        #create fft of images\n",
    "        fake_sim_fft = get_logf2(fake_sim)\n",
    "        fake_exp_fft = get_logf2(fake_exp)\n",
    "\n",
    "        #discriminate ffts\n",
    "        disc_fake_sim_fft = discriminator_sim_fft(fake_sim_fft, training=False)\n",
    "        disc_fake_exp_fft = discriminator_exp_fft(fake_exp_fft, training=False)\n",
    "\n",
    "        # calculate the loss\n",
    "        gen_exp_loss = (generator_loss(disc_fake_exp) + generator_loss(disc_fake_exp_fft))\n",
    "        gen_sim_loss = (generator_loss(disc_fake_sim) + generator_loss(disc_fake_sim_fft))\n",
    "\n",
    "        cycle_exp_loss = L1_loss(real_exp, cycled_exp)\n",
    "        cycle_sim_loss = L1_loss(real_sim, cycled_sim)\n",
    "        total_cycle_loss = (cycle_exp_loss + cycle_sim_loss)#/2\n",
    "\n",
    "        # identity loss\n",
    "        ident_exp_loss = L2_loss(real_exp, same_exp)\n",
    "        ident_sim_loss = L2_loss(real_sim, same_sim)\n",
    "\n",
    "        # Total generator loss = adversarial loss + cycle loss\n",
    "        total_gen_exp_loss = (gen_exp_loss + lam_cycle*total_cycle_loss + lam_ident*ident_exp_loss)#/(1 + lam_cycle + lam_ident)\n",
    "        total_gen_sim_loss = (gen_sim_loss + lam_cycle*total_cycle_loss + lam_ident*ident_sim_loss)#/(1 + lam_cycle + lam_ident)\n",
    "\n",
    "    if train:\n",
    "        # Calculate the gradients for generator and discriminator\n",
    "        generator_exp_gradients = tape.gradient(total_gen_exp_loss, generator_exp.trainable_variables)\n",
    "        generator_sim_gradients = tape.gradient(total_gen_sim_loss, generator_sim.trainable_variables)\n",
    "\n",
    "        # Apply the gradients to the optimizer\n",
    "        generator_exp_optimizer.apply_gradients(zip(generator_exp_gradients, generator_exp.trainable_variables))\n",
    "        generator_sim_optimizer.apply_gradients(zip(generator_sim_gradients, generator_sim.trainable_variables))\n",
    "\n",
    "    return cycle_exp_loss, cycle_sim_loss, gen_exp_loss, gen_sim_loss, ident_exp_loss, ident_sim_loss\n",
    "\n",
    "@tf.function\n",
    "def train_discriminators(real_sim, real_exp, train=True):\n",
    "    # persistent is set to True because the tape is used more than\n",
    "    # once to calculate the gradients.\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        # Generator G translates X -> Y\n",
    "        # Generator F translates Y -> X.\n",
    "\n",
    "        #generate fake images\n",
    "        fake_exp   = generator_exp(real_sim, training=False)\n",
    "        fake_sim   = generator_sim(real_exp, training=False)\n",
    "\n",
    "        #create ffts\n",
    "        fake_exp_fft = get_logf2(fake_exp)\n",
    "        fake_sim_fft = get_logf2(fake_sim)\n",
    "        real_exp_fft = get_logf2(real_exp)\n",
    "        real_sim_fft = get_logf2(real_sim)\n",
    "\n",
    "        #discriminate images\n",
    "        disc_real_sim = discriminator_sim(real_sim, training=train)\n",
    "        disc_fake_sim = discriminator_sim(fake_sim, training=train)\n",
    "\n",
    "        disc_real_exp = discriminator_exp(real_exp, training=train)\n",
    "        disc_fake_exp = discriminator_exp(fake_exp, training=train)\n",
    "\n",
    "        #discriminate ffts\n",
    "        disc_real_sim_fft = discriminator_sim_fft(real_sim_fft, training=train)\n",
    "        disc_fake_sim_fft = discriminator_sim_fft(fake_sim_fft, training=train)\n",
    "\n",
    "        disc_real_exp_fft = discriminator_exp_fft(real_exp_fft, training=train)\n",
    "        disc_fake_exp_fft = discriminator_exp_fft(fake_exp_fft, training=train)\n",
    "\n",
    "        # Calculate loss\n",
    "        disc_sim_loss = discriminator_loss(disc_real_sim, disc_fake_sim)\n",
    "        disc_exp_loss = discriminator_loss(disc_real_exp, disc_fake_exp)\n",
    "\n",
    "        disc_sim_loss_fft = discriminator_loss(disc_real_sim_fft, disc_fake_sim_fft)\n",
    "        disc_exp_loss_fft = discriminator_loss(disc_real_exp_fft, disc_fake_exp_fft)\n",
    "\n",
    "    if train:\n",
    "        # Calculate the gradients for generator and discriminator\n",
    "        discriminator_sim_gradients = tape.gradient(disc_sim_loss, discriminator_sim.trainable_variables)\n",
    "        discriminator_exp_gradients = tape.gradient(disc_exp_loss, discriminator_exp.trainable_variables)\n",
    "\n",
    "        discriminator_sim_gradients_fft = tape.gradient(disc_sim_loss_fft, discriminator_sim_fft.trainable_variables)\n",
    "        discriminator_exp_gradients_fft = tape.gradient(disc_exp_loss_fft, discriminator_exp_fft.trainable_variables)\n",
    "\n",
    "        # Apply the gradients to the optimizer\n",
    "        discriminator_sim_optimizer.apply_gradients(zip(discriminator_sim_gradients,\n",
    "                                                        discriminator_sim.trainable_variables))\n",
    "        discriminator_exp_optimizer.apply_gradients(zip(discriminator_exp_gradients,\n",
    "                                                        discriminator_exp.trainable_variables))\n",
    "\n",
    "        discriminator_sim_fft_optimizer.apply_gradients(zip(discriminator_sim_gradients_fft,\n",
    "                                                            discriminator_sim_fft.trainable_variables))\n",
    "        discriminator_exp_fft_optimizer.apply_gradients(zip(discriminator_exp_gradients_fft,\n",
    "                                                            discriminator_exp_fft.trainable_variables))\n",
    "\n",
    "    return disc_sim_loss, disc_exp_loss, disc_sim_loss_fft, disc_exp_loss_fft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the models\n",
    "\n",
    "Finally we train the models and save checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_exp_losses,   gen_sim_losses   = [], []\n",
    "disc_exp_losses,  disc_sim_losses  = [], []\n",
    "disc_exp_fft_losses,  disc_sim_fft_losses  = [], []\n",
    "cycle_exp_losses, cycle_sim_losses = [], []\n",
    "dist_gen_exp_losses, dist_gen_sim_losses = [], []\n",
    "disc_exp_accs,    disc_sim_accs    = [], []\n",
    "epoch = 0\n",
    "batch_size = 42\n",
    "train_size = 2478\n",
    "\n",
    "data_sim = glob('{}/*.*'.format(sim_dir))\n",
    "data_exp = glob('{}/*.*'.format(exp_dir))\n",
    "\n",
    "if len(data_sim) == 0:\n",
    "    print(\"ERROR\")\n",
    "    print(sim_dir)\n",
    "    exit()\n",
    "\n",
    "if len(data_exp) == 0:\n",
    "    print(\"ERROR\")\n",
    "    print(exp_dir)\n",
    "    exit()\n",
    "\n",
    "batch_idxs = min(min(len(data_sim), len(data_exp)), train_size) // batch_size\n",
    "\n",
    "total_epochs = 1e6//(batch_idxs*batch_size)\n",
    "epoch_step = total_epochs//2\n",
    "\n",
    "print(\"batch_idxs = \", batch_idxs)\n",
    "print(\"total epochs = \", total_epochs)\n",
    "\n",
    "try:\n",
    "    ( gen_exp_losses,   gen_sim_losses,\n",
    "     disc_exp_losses,  disc_sim_losses,\n",
    "     disc_exp_fft_losses,  disc_sim_fft_losses,\n",
    "     cycle_exp_losses, cycle_sim_losses,\n",
    "     dist_gen_exp_losses, dist_gen_sim_losses,\n",
    "     disc_exp_accs,    disc_sim_accs,\n",
    "     epoch) = pickle.load(open(log_data_fn, \"rb\" ))\n",
    "    print(\"loading at epoch \", epoch)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    pass\n",
    "last=None\n",
    "\n",
    "ls = ls0\n",
    "le = le0\n",
    "\n",
    "while epoch < total_epochs:\n",
    "    start = time.time()\n",
    "    lr = lr0*(1 - (epoch - epoch_step)/(total_epochs-1 - epoch_step)) if epoch >= epoch_step else lr0\n",
    "    print(epoch, lr, ls, le)\n",
    "    generator_exp_optimizer.lr.assign(lr)\n",
    "    generator_sim_optimizer.lr.assign(lr)\n",
    "    discriminator_sim_optimizer.lr.assign(ls)\n",
    "    discriminator_exp_optimizer.lr.assign(le)\n",
    "    \n",
    "    discriminator_sim_fft_optimizer.lr.assign(lr)\n",
    "    discriminator_exp_fft_optimizer.lr.assign(lr)\n",
    "\n",
    "    np.random.shuffle(data_sim)\n",
    "    np.random.shuffle(data_exp)\n",
    "\n",
    "    dis_losses, gen_losses = np.zeros(4), np.zeros(6)\n",
    "    for idx in range(0, batch_idxs):\n",
    "        batch_start =  time.time()\n",
    "        sim_fn_list = data_sim[idx*batch_size:(idx + 1)*batch_size]\n",
    "        exp_fn_list = data_exp[idx*batch_size:(idx + 1)*batch_size]\n",
    "\n",
    "        real_sim = np.array([load_train_data(fn, num_channels) for fn in sim_fn_list]).astype(np.float32)\n",
    "        real_exp = np.array([load_train_data(fn, num_channels) for fn in exp_fn_list]).astype(np.float32)\n",
    "\n",
    "        dis_losses += train_discriminators(real_sim, real_exp, train=True)\n",
    "        gen_losses += train_generators(real_sim, real_exp, train=True)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        fake_sim = generator_sim(real_exp, training=False)\n",
    "        fake_exp = generator_exp(real_sim, training=False)\n",
    "\n",
    "        disc_sim_acc = get_discriminator_acc(discriminator_sim, real_sim, fake_sim, threshold=0.5, with_logits=True, verbose=False)\n",
    "        disc_exp_acc = get_discriminator_acc(discriminator_exp, real_exp, fake_exp, threshold=0.5, with_logits=True, verbose=False)\n",
    "\n",
    "        ls = lr*2*abs(1-disc_sim_acc)\n",
    "        le = lr*2*abs(1-disc_exp_acc)\n",
    "\n",
    "        print(\"[{}/{}]: {}, ({},{})\".format(idx, batch_idxs, (time.time()-batch_start)/batch_size, ls, le))\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    cycle_exp_loss, cycle_sim_loss, gen_exp_loss, gen_sim_loss, dist_gen_exp_loss, dist_gen_sim_loss= gen_losses/batch_idxs\n",
    "    disc_sim_loss, disc_exp_loss, disc_sim_loss_fft, disc_exp_loss_fft = dis_losses/batch_idxs\n",
    "\n",
    "    # Calculate accuracy\n",
    "    fake_sim = generator_sim(real_exp, training=False)\n",
    "    fake_exp = generator_exp(real_sim, training=False)\n",
    "\n",
    "    disc_sim_acc = get_discriminator_acc(discriminator_sim, real_sim, fake_sim, threshold=0.5, with_logits=True)\n",
    "    disc_exp_acc = get_discriminator_acc(discriminator_exp, real_exp, fake_exp, threshold=0.5, with_logits=True)\n",
    "\n",
    "    ls = lr*2*abs(1-disc_sim_acc)\n",
    "    le = lr*2*abs(1-disc_exp_acc)\n",
    "\n",
    "\n",
    "    gen_exp_losses.append(gen_exp_loss)\n",
    "    gen_sim_losses.append(gen_sim_loss)\n",
    "    cycle_exp_losses.append(cycle_exp_loss)\n",
    "    cycle_sim_losses.append(cycle_sim_loss)\n",
    "    dist_gen_exp_losses.append(dist_gen_exp_loss)\n",
    "    dist_gen_sim_losses.append(dist_gen_sim_loss)\n",
    "    disc_sim_losses.append(disc_sim_loss)\n",
    "    disc_exp_losses.append(disc_exp_loss)\n",
    "    disc_sim_fft_losses.append(disc_sim_loss_fft)\n",
    "    disc_exp_fft_losses.append(disc_exp_loss_fft)\n",
    "    disc_sim_accs.append(disc_sim_acc)\n",
    "    disc_exp_accs.append(disc_exp_acc)\n",
    "    epoch += 1\n",
    "\n",
    "    sample_sim = real_sim[0].reshape([1,fine_size,fine_size,num_channels])\n",
    "    sample_exp = real_exp[0].reshape([1,fine_size,fine_size,1])\n",
    "    generate_images(generator_exp, generator_sim, sample_sim)\n",
    "    generate_images(generator_sim, generator_exp, sample_exp)\n",
    "    generate_losses(gen_exp_losses,   gen_sim_losses,\n",
    "                    cycle_exp_losses, cycle_sim_losses,\n",
    "                    dist_gen_exp_losses, dist_gen_sim_losses,\n",
    "                    disc_sim_losses,  disc_exp_losses,   \n",
    "                    disc_sim_fft_losses,  disc_exp_fft_losses, \n",
    "                    epoch, last=last)\n",
    "    generate_accuracies(disc_sim_accs, disc_exp_accs, epoch, last=last)\n",
    "\n",
    "    if (epoch) % 10 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch, ckpt_save_path))\n",
    "        pickle.dump( (    gen_exp_losses,   gen_sim_losses,\n",
    "                         disc_exp_losses,  disc_sim_losses,\n",
    "                      disc_exp_fft_losses,  disc_sim_fft_losses,\n",
    "                        cycle_exp_losses, cycle_sim_losses,\n",
    "                        dist_gen_exp_losses, dist_gen_sim_losses,\n",
    "                         disc_exp_accs,    disc_sim_accs,\n",
    "                        epoch-1),\n",
    "                     open( log_data_fn, \"wb\" ) )\n",
    "\n",
    "    print ('Time taken for epoch {} is {} sec\\n'.format(epoch, time.time()-start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_save_path = ckpt_manager.save()\n",
    "print ('Saving checkpoint for epoch {} at {}'.format(epoch, ckpt_save_path))\n",
    "pickle.dump( (    gen_exp_losses,   gen_sim_losses,\n",
    "               disc_exp_losses,  disc_sim_losses,\n",
    "            disc_exp_fft_losses,  disc_sim_fft_losses,\n",
    "            cycle_exp_losses, cycle_sim_losses,\n",
    "            dist_gen_exp_losses, dist_gen_sim_losses,\n",
    "               disc_exp_accs,    disc_sim_accs,\n",
    "            epoch-1),\n",
    "         open( log_data_fn, \"wb\" ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
